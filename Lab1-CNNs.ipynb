{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97f7c5d-46f3-4cbd-80ad-f1e50cd65096",
   "metadata": {},
   "source": [
    "# Deep Learning Applications: Laboratory #1\n",
    "\n",
    "In this first laboratory we will work relatively simple architectures to get a feel for working with Deep Models. This notebook is designed to work with PyTorch, but as I said in the introductory lecture: please feel free to use and experiment with whatever tools you like.\n",
    "\n",
    "**Important Notes**:\n",
    "1. Be sure to **document** all of your decisions, as well as your intermediate and final results. Make sure your conclusions and analyses are clearly presented. Don't make us dig into your code or walls of printed results to try to draw conclusions from your code.\n",
    "2. If you use code from someone else (e.g. Github, Stack Overflow, ChatGPT, etc) you **must be transparent about it**. Document your sources and explain how you adapted any partial solutions to creat **your** solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed8906-bd19-4b4f-8b79-4feae355ffd6",
   "metadata": {},
   "source": [
    "## Exercise 1: Warming Up\n",
    "In this series of exercises I want you to try to duplicate (on a small scale) the results of the ResNet paper:\n",
    "\n",
    "> [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385), Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR 2016.\n",
    "\n",
    "We will do this in steps using a Multilayer Perceptron on MNIST.\n",
    "\n",
    "Recall that the main message of the ResNet paper is that **deeper** networks do not **guarantee** more reduction in training loss (or in validation accuracy). Below you will incrementally build a sequence of experiments to verify this for an MLP. A few guidelines:\n",
    "\n",
    "+ I have provided some **starter** code at the beginning. **NONE** of this code should survive in your solutions. Not only is it **very** badly written, it is also written in my functional style that also obfuscates what it's doing (in part to **discourage** your reuse!). It's just to get you *started*.\n",
    "+ These exercises ask you to compare **multiple** training runs, so it is **really** important that you factor this into your **pipeline**. Using [Tensorboard](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) is a **very** good idea -- or, even better [Weights and Biases](https://wandb.ai/site).\n",
    "+ You may work and submit your solutions in **groups of at most two**. Share your ideas with everyone, but the solutions you submit *must be your own*.\n",
    "\n",
    "First some boilerplate to get you started, then on to the actual exercises!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb2b6d1-3df0-464c-9a5f-8c611257a971",
   "metadata": {},
   "source": [
    "### Preface: Some code to get you started\n",
    "\n",
    "What follows is some **very simple** code for training an MLP on MNIST. The point of this code is to get you up and running (and to verify that your Python environment has all needed dependencies).\n",
    "\n",
    "**Note**: As you read through my code and execute it, this would be a good time to think about *abstracting* **your** model definition, and training and evaluation pipelines in order to make it easier to compare performance of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab3a8282-2322-4dca-b76e-2f3863bc75fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "#import copy\n",
    "#from functools import reduce\n",
    "\n",
    "# Numerical and plotting\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch core\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.models as models\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Experiment tracking\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cc12cc-8422-47bf-8d8e-0950ac05ae96",
   "metadata": {},
   "source": [
    "#### Data preparation\n",
    "\n",
    "Here is some basic dataset loading, validation splitting code to get you started working with MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "272a69db-0416-444a-9be4-5f055ff48bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard MNIST transform.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load MNIST train and test.\n",
    "ds_train = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "ds_test = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split train into train and validation.\n",
    "val_size = 5000\n",
    "I = np.random.permutation(len(ds_train))\n",
    "ds_val = Subset(ds_train, I[:val_size])\n",
    "ds_train = Subset(ds_train, I[val_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2cad13-ee2c-4e43-b5c7-31760da8c2df",
   "metadata": {},
   "source": [
    "### Exercise 1.1: A baseline MLP\n",
    "\n",
    "Implement a *simple* Multilayer Perceptron to classify the 10 digits of MNIST (e.g. two *narrow* layers). Use my code above as inspiration, but implement your own training pipeline -- you will need it later. Train this model to convergence, monitoring (at least) the loss and accuracy on the training and validation sets for every epoch. Below I include a basic implementation to get you started -- remember that you should write your *own* pipeline!\n",
    "\n",
    "**Note**: This would be a good time to think about *abstracting* your model definition, and training and evaluation pipelines in order to make it easier to compare performance of different models.\n",
    "\n",
    "**Important**: Given the *many* runs you will need to do, and the need to *compare* performance between them, this would **also** be a great point to study how **Tensorboard** or **Weights and Biases** can be used for performance monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c14afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96128a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "batch_size = 128\n",
    "dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dl_val   = DataLoader(ds_val,   batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "dl_test  = DataLoader(ds_test,  batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c469368",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size=28*28, hidden1=128, hidden2=64, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_size, hidden1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden2, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f33194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dl, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in tqdm(dl, desc=\"Batches\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return running_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2fdfee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dl, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    preds_all = []\n",
    "    gts_all = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(dl, desc=\"Batches\", leave=False):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits, y, reduction='sum')\n",
    "            running_loss += loss.item()\n",
    "            preds_all.append(logits.argmax(dim=1).cpu().numpy())\n",
    "            gts_all.append(y.cpu().numpy())\n",
    "    preds_all = np.hstack(preds_all)\n",
    "    gts_all = np.hstack(gts_all)\n",
    "    return running_loss / len(gts_all), accuracy_score(gts_all, preds_all), classification_report(gts_all, preds_all, zero_division=0, digits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79b8403a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatteo-piras\u001b[0m (\u001b[33mmatteo-piras-universit-di-firenze\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/andromeda/personal/mpiras/Lab_1/wandb/run-20251117_105755-xxvd5uoh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/xxvd5uoh' target=\"_blank\">hearty-morning-7</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/xxvd5uoh' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/xxvd5uoh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: train_loss=0.3480 train_acc=0.9004 | val_loss=0.2000 val_acc=0.9396\n",
      "Epoch 02: train_loss=0.1462 train_acc=0.9564 | val_loss=0.1458 val_acc=0.9568\n",
      "Epoch 03: train_loss=0.1013 train_acc=0.9692 | val_loss=0.1141 val_acc=0.9650\n",
      "Epoch 04: train_loss=0.0752 train_acc=0.9769 | val_loss=0.1165 val_acc=0.9658\n",
      "Epoch 05: train_loss=0.0584 train_acc=0.9820 | val_loss=0.0939 val_acc=0.9734\n",
      "Epoch 06: train_loss=0.0489 train_acc=0.9845 | val_loss=0.0925 val_acc=0.9742\n",
      "Epoch 07: train_loss=0.0378 train_acc=0.9877 | val_loss=0.1183 val_acc=0.9670\n",
      "Epoch 08: train_loss=0.0323 train_acc=0.9892 | val_loss=0.1053 val_acc=0.9698\n",
      "Epoch 09: train_loss=0.0264 train_acc=0.9911 | val_loss=0.1076 val_acc=0.9726\n",
      "Epoch 10: train_loss=0.0225 train_acc=0.9926 | val_loss=0.1141 val_acc=0.9722\n",
      "Epoch 11: train_loss=0.0197 train_acc=0.9935 | val_loss=0.1123 val_acc=0.9742\n",
      "Epoch 12: train_loss=0.0176 train_acc=0.9942 | val_loss=0.1163 val_acc=0.9746\n",
      "Epoch 13: train_loss=0.0169 train_acc=0.9944 | val_loss=0.1235 val_acc=0.9740\n",
      "Epoch 14: train_loss=0.0147 train_acc=0.9951 | val_loss=0.1316 val_acc=0.9754\n",
      "Epoch 15: train_loss=0.0137 train_acc=0.9955 | val_loss=0.1317 val_acc=0.9708\n",
      "Epoch 16: train_loss=0.0163 train_acc=0.9944 | val_loss=0.1382 val_acc=0.9720\n",
      "Epoch 17: train_loss=0.0118 train_acc=0.9961 | val_loss=0.1431 val_acc=0.9732\n",
      "Epoch 18: train_loss=0.0129 train_acc=0.9953 | val_loss=0.1329 val_acc=0.9756\n",
      "Epoch 19: train_loss=0.0074 train_acc=0.9977 | val_loss=0.1530 val_acc=0.9732\n",
      "Epoch 20: train_loss=0.0090 train_acc=0.9968 | val_loss=0.1537 val_acc=0.9734\n"
     ]
    }
   ],
   "source": [
    "model = SimpleMLP(hidden1=128, hidden2=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 20\n",
    "\n",
    "train_losses, train_accs = [], []\n",
    "val_losses, val_accs = [], []\n",
    "best_val_acc = 0.0\n",
    "best_state = None\n",
    "\n",
    "wandb.init(project=\"Lab-1\", config={\n",
    "\"epochs\": epochs,\n",
    "\"lr\": 1e-3,\n",
    "\"batch_size\": batch_size,\n",
    "\"model\": \"SimpleMLP\"\n",
    "})\n",
    "config = wandb.config\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    train_loss, train_acc = train_one_epoch(model, dl_train, optimizer, device)\n",
    "    val_loss, val_acc, _ = evaluate(model, dl_val, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    wandb.log({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"train_accuracy\": train_acc,\n",
    "        \"val_accuracy\": val_acc\n",
    "    }, step=ep)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    print(f\"Epoch {ep:02d}: train_loss={train_loss:.4f} train_acc={train_acc:.4f} | val_loss={val_loss:.4f} val_acc={val_acc:.4f}\")\n",
    "\n",
    "artifact = wandb.Artifact(\"simple-mlp\", type=\"model\")\n",
    "artifact.add_file(\"best_model.pth\")\n",
    "wandb.log_artifact(artifact)\n",
    "best_state = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09d34d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇▇▇████████████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆██▆▇▇▇████▇▇████</td></tr><tr><td>val_loss</td><td>█▄▂▃▁▁▃▂▂▂▂▃▃▄▄▄▄▄▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>classification_report</td><td>              precis...</td></tr><tr><td>test_acc</td><td>0.978</td></tr><tr><td>test_loss</td><td>0.1148</td></tr><tr><td>train_accuracy</td><td>0.99678</td></tr><tr><td>train_loss</td><td>0.00903</td></tr><tr><td>val_accuracy</td><td>0.9734</td></tr><tr><td>val_loss</td><td>0.15369</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hearty-morning-7</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/xxvd5uoh' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/xxvd5uoh</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a><br>Synced 4 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251117_105755-xxvd5uoh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.1148  Test acc: 0.9780\n",
      "Classification report on TEST:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.984     0.988     0.986       980\n",
      "           1      0.989     0.994     0.992      1135\n",
      "           2      0.962     0.988     0.975      1032\n",
      "           3      0.967     0.983     0.975      1010\n",
      "           4      0.970     0.986     0.978       982\n",
      "           5      0.978     0.964     0.971       892\n",
      "           6      0.980     0.974     0.977       958\n",
      "           7      0.982     0.975     0.979      1028\n",
      "           8      0.985     0.959     0.972       974\n",
      "           9      0.982     0.965     0.974      1009\n",
      "\n",
      "    accuracy                          0.978     10000\n",
      "   macro avg      0.978     0.978     0.978     10000\n",
      "weighted avg      0.978     0.978     0.978     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "\n",
    "# Final test evaluation\n",
    "test_loss, test_acc, test_report = evaluate(model, dl_test, device)\n",
    "wandb.log({\"test_loss\": test_loss, \"test_acc\": test_acc})\n",
    "wandb.log({\"classification_report\": str(test_report)})\n",
    "wandb.finish()\n",
    "print(f\"Test loss: {test_loss:.4f}  Test acc: {test_acc:.4f}\")\n",
    "print(\"Classification report on TEST:\\n\", test_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8ad9b-e3ae-4c49-9bec-35aaea149b08",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Adding Residual Connections\n",
    "\n",
    "Implement a variant of your parameterized MLP network to support **residual** connections. Your network should be defined as a composition of **residual MLP** blocks that have one or more linear layers and add a skip connection from the block input to the output of the final linear layer.\n",
    "\n",
    "**Compare** the performance (in training/validation loss and test accuracy) of your MLP and ResidualMLP for a range of depths. Verify that deeper networks **with** residual connections are easier to train than a network of the same depth **without** residual connections.\n",
    "\n",
    "**For extra style points**: See if you can explain by analyzing the gradient magnitudes on a single training batch *why* this is the case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0c4488",
   "metadata": {},
   "source": [
    "#### General Model Overview\n",
    "\n",
    "The `GeneralModel` class implements a flexible neural network architecture composed of:\n",
    "\n",
    "- **A sequence of user-defined blocks** (e.g., MLP layers, convolutional layers, residual blocks).  \n",
    "- **A final classifier**, typically a linear layer.\n",
    "\n",
    "The model includes two flattening mechanisms:\n",
    "\n",
    "- **Optional input flattening** for fully connected architectures.  \n",
    "- **Automatic output flattening** when the last block returns multi-dimensional tensors.\n",
    "\n",
    "This design allows building MLPs, CNNs, or hybrid architectures without manual shape management.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7f9a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralModel(nn.Module):\n",
    "    def __init__(self, blocks, classifier, flatten_input=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Optional flattening before blocks (for MLPs)\n",
    "        self.input_flatten = nn.Flatten() if flatten_input else nn.Identity()\n",
    "\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Optional flatten before blocks\n",
    "        x = self.input_flatten(x)\n",
    "\n",
    "        # Apply blocks\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # Automatically flatten before classifier if needed\n",
    "        if x.ndim > 2:\n",
    "            x = x.view(x.size(0), -1)\n",
    "\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a4b0a4",
   "metadata": {},
   "source": [
    "#### Residual MLP Block Overview\n",
    "\n",
    "This block is a **flexible MLP version of a ResNet block**:\n",
    "\n",
    "- Supports **one or multiple linear layers**.\n",
    "- Handles **dimension changes** via a **projection** in the skip connection.\n",
    "- During the construction of the `Sequential` block, it **skips the ReLU after the last layer**:\n",
    "  - Ensures the **final ReLU is applied after adding the residual connection**, consistent with standard ResNet design.\n",
    "- Allows **disabling the skip connection**, producing an **identical architecture without residuals** for clean comparisons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0df10e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code genrated by AI\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, hidden_layers=2, use_skip=True):\n",
    "        super().__init__()\n",
    "        self.use_skip = use_skip\n",
    "\n",
    "        # Skip connection projection (if needed)\n",
    "        if use_skip:\n",
    "            self.proj = nn.Linear(dim_in, dim_out) if dim_in != dim_out else nn.Identity()\n",
    "\n",
    "        # Build MLP layers\n",
    "        layers = [nn.Linear(dim_in, dim_out)]\n",
    "        for _ in range(hidden_layers - 2):\n",
    "            layers.append(nn.Linear(dim_out, dim_out))\n",
    "        if hidden_layers > 1:\n",
    "            layers.append(nn.Linear(dim_out, dim_out))\n",
    "\n",
    "        self.net = nn.Sequential(*[\n",
    "            nn.Sequential(l, nn.ReLU(inplace=True)) if i < hidden_layers - 1 else l\n",
    "            for i, l in enumerate(layers)\n",
    "        ])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        if self.use_skip:\n",
    "            skip = self.proj(x)\n",
    "            return self.relu(out + skip)\n",
    "        else:\n",
    "            return self.relu(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab275ee9",
   "metadata": {},
   "source": [
    "As a personal reminder, this code:  \n",
    "```python\n",
    "self.net = nn.Sequential(*[\n",
    "    nn.Sequential(l, nn.ReLU(inplace=True)) if i < hidden_layers - 1 else l\n",
    "    for i, l in enumerate(layers)\n",
    "])\n",
    "```\n",
    "is the compact version of this code:\n",
    "```python\n",
    "layers_with_activation = []\n",
    "\n",
    "# Loop through all layers\n",
    "for i, layer in enumerate(layers):\n",
    "\n",
    "    # If this is NOT the last layer → add ReLU after it\n",
    "    if i < hidden_layers - 1:\n",
    "        block = nn.Sequential(\n",
    "            layer,\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    else:\n",
    "        # Last layer: no activation\n",
    "        block = layer\n",
    "\n",
    "    layers_with_activation.append(block)\n",
    "\n",
    "# Build nn.Sequential using unpacking\n",
    "self.net = nn.Sequential(*layers_with_activation)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c747e1fd",
   "metadata": {},
   "source": [
    "#### Comparing three different configurations 2, 4 and 8 residual vs non-residual blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c060acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\n",
    "    \"batch_size\": 128,\n",
    "    \"lr\": 0.01,\n",
    "    \"epochs\": 10,\n",
    "    \"depths\": [[784, 128, 128, 10],[784, 128, 128, 128, 128, 10],[784, 128, 128, 128 , 128, 128, 128, 128 , 128, 10] ],  # example depths\n",
    "    \"block_hidden_layers\": 2,\n",
    "    \"use_skip_options\": [False, True]\n",
    "}\n",
    "\n",
    "# Dataloaders.\n",
    "train_loader = torch.utils.data.DataLoader(ds_train, config[\"batch_size\"], shuffle=True, num_workers=4)\n",
    "val_loader   = torch.utils.data.DataLoader(ds_val, config[\"batch_size\"], num_workers=4)\n",
    "dl_test  = DataLoader(ds_test,  batch_size=config[\"batch_size\"], shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c923da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiment: depth_2_skip_False ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatteo-piras\u001b[0m (\u001b[33mmatteo-piras-universit-di-firenze\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/andromeda/personal/mpiras/Lab_1/wandb/run-20251118_090510-ro434xc4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/ro434xc4' target=\"_blank\">depth_2_skip_False</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/ro434xc4' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/ro434xc4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[depth_2_skip_False] Epoch 0: \n",
      "[depth_2_skip_False] Epoch 1: \n",
      "[depth_2_skip_False] Epoch 2: \n",
      "[depth_2_skip_False] Epoch 3: \n",
      "[depth_2_skip_False] Epoch 4: \n",
      "[depth_2_skip_False] Epoch 5: \n",
      "[depth_2_skip_False] Epoch 6: \n",
      "[depth_2_skip_False] Epoch 7: \n",
      "[depth_2_skip_False] Epoch 8: \n",
      "[depth_2_skip_False] Epoch 9: \n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>grad_first_layer</td><td>█▄▅▂▄▇▁▅▆▁</td></tr><tr><td>grad_norm_max</td><td>█▄▅▂▄▇▁▅▆▁</td></tr><tr><td>grad_norm_mean</td><td>█▄▃▁▃▅▁▄▄▁</td></tr><tr><td>grad_norm_min</td><td>█▃▂▁▂▄▁▂▃▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁▇▇▇██████</td></tr><tr><td>train_loss</td><td>█▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇██████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>classification_report</td><td>              precis...</td></tr><tr><td>grad_first_layer</td><td>0.28019</td></tr><tr><td>grad_norm_max</td><td>0.28019</td></tr><tr><td>grad_norm_mean</td><td>0.06919</td></tr><tr><td>grad_norm_min</td><td>0.00342</td></tr><tr><td>test_acc</td><td>0.9741</td></tr><tr><td>test_loss</td><td>0.09587</td></tr><tr><td>train_accuracy</td><td>0.9932</td></tr><tr><td>train_loss</td><td>0.02297</td></tr><tr><td>val_accuracy</td><td>0.9774</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">depth_2_skip_False</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/ro434xc4' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/ro434xc4</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251118_090510-ro434xc4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiment: depth_2_skip_True ===\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/andromeda/personal/mpiras/Lab_1/wandb/run-20251118_090559-l2y3mrrg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/l2y3mrrg' target=\"_blank\">depth_2_skip_True</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/l2y3mrrg' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/l2y3mrrg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[depth_2_skip_True] Epoch 0: \n",
      "[depth_2_skip_True] Epoch 1: \n",
      "[depth_2_skip_True] Epoch 2: \n",
      "[depth_2_skip_True] Epoch 3: \n",
      "[depth_2_skip_True] Epoch 4: \n",
      "[depth_2_skip_True] Epoch 5: \n",
      "[depth_2_skip_True] Epoch 6: \n",
      "[depth_2_skip_True] Epoch 7: \n",
      "[depth_2_skip_True] Epoch 8: \n",
      "[depth_2_skip_True] Epoch 9: \n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>grad_first_layer</td><td>▄▄▅█▁▅▄▃▃▅</td></tr><tr><td>grad_norm_max</td><td>▄▄▅█▁▅▄▃▃▅</td></tr><tr><td>grad_norm_mean</td><td>▄▄▅█▁▅▄▃▃▄</td></tr><tr><td>grad_norm_min</td><td>▄▄▅█▁▆▅▃▃▄</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇▇▇████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>classification_report</td><td>              precis...</td></tr><tr><td>grad_first_layer</td><td>0.64166</td></tr><tr><td>grad_norm_max</td><td>0.64166</td></tr><tr><td>grad_norm_mean</td><td>0.17522</td></tr><tr><td>grad_norm_min</td><td>0.01075</td></tr><tr><td>test_acc</td><td>0.979</td></tr><tr><td>test_loss</td><td>0.07208</td></tr><tr><td>train_accuracy</td><td>0.99522</td></tr><tr><td>train_loss</td><td>0.01707</td></tr><tr><td>val_accuracy</td><td>0.9816</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">depth_2_skip_True</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/l2y3mrrg' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/l2y3mrrg</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251118_090559-l2y3mrrg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiment: depth_4_skip_False ===\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/andromeda/personal/mpiras/Lab_1/wandb/run-20251118_090645-ht0104ll</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/ht0104ll' target=\"_blank\">depth_4_skip_False</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/ht0104ll' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/ht0104ll</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[depth_4_skip_False] Epoch 0: \n",
      "[depth_4_skip_False] Epoch 1: \n",
      "[depth_4_skip_False] Epoch 2: \n",
      "[depth_4_skip_False] Epoch 3: \n",
      "[depth_4_skip_False] Epoch 4: \n",
      "[depth_4_skip_False] Epoch 5: \n",
      "[depth_4_skip_False] Epoch 6: \n",
      "[depth_4_skip_False] Epoch 7: \n",
      "[depth_4_skip_False] Epoch 8: \n",
      "[depth_4_skip_False] Epoch 9: \n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>grad_first_layer</td><td>▁▂▆█▃▅▆▄▃▇</td></tr><tr><td>grad_norm_max</td><td>▁▃▅█▃▅▆▄▃▇</td></tr><tr><td>grad_norm_mean</td><td>▁▃▄█▃▅▅▄▂▇</td></tr><tr><td>grad_norm_min</td><td>▁▆▄█▄▆▅▄▂▅</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁▁▆███████</td></tr><tr><td>train_loss</td><td>██▄▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▂████████</td></tr><tr><td>val_loss</td><td>█▇▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>classification_report</td><td>              precis...</td></tr><tr><td>grad_first_layer</td><td>1.71414</td></tr><tr><td>grad_norm_max</td><td>1.71414</td></tr><tr><td>grad_norm_mean</td><td>0.38258</td></tr><tr><td>grad_norm_min</td><td>0.01744</td></tr><tr><td>test_acc</td><td>0.9578</td></tr><tr><td>test_loss</td><td>0.15878</td></tr><tr><td>train_accuracy</td><td>0.98376</td></tr><tr><td>train_loss</td><td>0.05679</td></tr><tr><td>val_accuracy</td><td>0.971</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">depth_4_skip_False</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/ht0104ll' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/ht0104ll</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251118_090645-ht0104ll/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiment: depth_4_skip_True ===\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/andromeda/personal/mpiras/Lab_1/wandb/run-20251118_090732-tvjf3yi5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/tvjf3yi5' target=\"_blank\">depth_4_skip_True</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/tvjf3yi5' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/tvjf3yi5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[depth_4_skip_True] Epoch 0: \n",
      "[depth_4_skip_True] Epoch 1: \n",
      "[depth_4_skip_True] Epoch 2: \n",
      "[depth_4_skip_True] Epoch 3: \n",
      "[depth_4_skip_True] Epoch 4: \n",
      "[depth_4_skip_True] Epoch 5: \n",
      "[depth_4_skip_True] Epoch 6: \n",
      "[depth_4_skip_True] Epoch 7: \n",
      "[depth_4_skip_True] Epoch 8: \n",
      "[depth_4_skip_True] Epoch 9: \n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>grad_first_layer</td><td>▅▄▄▆█▂▂▃▂▁</td></tr><tr><td>grad_norm_max</td><td>▅▄▄▆█▂▂▃▂▁</td></tr><tr><td>grad_norm_mean</td><td>▄▅▄▆█▂▂▃▃▁</td></tr><tr><td>grad_norm_min</td><td>▃▄▃▆█▁▂▂▂▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆█▇██▇█</td></tr><tr><td>val_loss</td><td>█▅▃▃▂▂▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>classification_report</td><td>              precis...</td></tr><tr><td>grad_first_layer</td><td>0.3685</td></tr><tr><td>grad_norm_max</td><td>0.3685</td></tr><tr><td>grad_norm_mean</td><td>0.0787</td></tr><tr><td>grad_norm_min</td><td>0.00617</td></tr><tr><td>test_acc</td><td>0.9777</td></tr><tr><td>test_loss</td><td>0.07709</td></tr><tr><td>train_accuracy</td><td>0.99556</td></tr><tr><td>train_loss</td><td>0.01441</td></tr><tr><td>val_accuracy</td><td>0.9802</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">depth_4_skip_True</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/tvjf3yi5' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/tvjf3yi5</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251118_090732-tvjf3yi5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiment: depth_8_skip_False ===\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/andromeda/personal/mpiras/Lab_1/wandb/run-20251118_090819-7afbbcob</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/7afbbcob' target=\"_blank\">depth_8_skip_False</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/7afbbcob' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/7afbbcob</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[depth_8_skip_False] Epoch 0: \n",
      "[depth_8_skip_False] Epoch 1: \n",
      "[depth_8_skip_False] Epoch 2: \n",
      "[depth_8_skip_False] Epoch 3: \n",
      "[depth_8_skip_False] Epoch 4: \n",
      "[depth_8_skip_False] Epoch 5: \n",
      "[depth_8_skip_False] Epoch 6: \n",
      "[depth_8_skip_False] Epoch 7: \n",
      "[depth_8_skip_False] Epoch 8: \n",
      "[depth_8_skip_False] Epoch 9: \n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>grad_first_layer</td><td>▃▄▃▁▄▅█▇▃▁</td></tr><tr><td>grad_norm_max</td><td>▅▄▁▂▃▆▅█▃▁</td></tr><tr><td>grad_norm_mean</td><td>▄▅▁▂▃▆▅█▃▁</td></tr><tr><td>grad_norm_min</td><td>▄▃▂▁▄▆▇█▃▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁█████████</td></tr><tr><td>train_loss</td><td>█▃▁▂▃▁▃▂▃▂</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄▄▅▄▁█▃▆▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>classification_report</td><td>              precis...</td></tr><tr><td>grad_first_layer</td><td>0.0</td></tr><tr><td>grad_norm_max</td><td>0.05391</td></tr><tr><td>grad_norm_mean</td><td>0.0032</td></tr><tr><td>grad_norm_min</td><td>0.0</td></tr><tr><td>test_acc</td><td>0.1135</td></tr><tr><td>test_loss</td><td>2.30113</td></tr><tr><td>train_accuracy</td><td>0.11193</td></tr><tr><td>train_loss</td><td>2.3015</td></tr><tr><td>val_accuracy</td><td>0.1172</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">depth_8_skip_False</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/7afbbcob' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/7afbbcob</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251118_090819-7afbbcob/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiment: depth_8_skip_True ===\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/andromeda/personal/mpiras/Lab_1/wandb/run-20251118_090915-ry51vnt0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/ry51vnt0' target=\"_blank\">depth_8_skip_True</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/ry51vnt0' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/ry51vnt0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[depth_8_skip_True] Epoch 0: \n",
      "[depth_8_skip_True] Epoch 1: \n",
      "[depth_8_skip_True] Epoch 2: \n",
      "[depth_8_skip_True] Epoch 3: \n",
      "[depth_8_skip_True] Epoch 4: \n",
      "[depth_8_skip_True] Epoch 5: \n",
      "[depth_8_skip_True] Epoch 6: \n",
      "[depth_8_skip_True] Epoch 7: \n",
      "[depth_8_skip_True] Epoch 8: \n",
      "[depth_8_skip_True] Epoch 9: \n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>grad_first_layer</td><td>▄██▇▁▆▅▂▅▁</td></tr><tr><td>grad_norm_max</td><td>▄██▇▁▆▅▂▅▁</td></tr><tr><td>grad_norm_mean</td><td>▄██▆▁▅▅▂▅▁</td></tr><tr><td>grad_norm_min</td><td>▄█▆▅▁▅▄▂▄▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁▆▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▅▇▇▆██▅█</td></tr><tr><td>val_loss</td><td>█▇▃▂▂▃▁▁▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>classification_report</td><td>              precis...</td></tr><tr><td>grad_first_layer</td><td>0.04275</td></tr><tr><td>grad_norm_max</td><td>0.04275</td></tr><tr><td>grad_norm_mean</td><td>0.00655</td></tr><tr><td>grad_norm_min</td><td>0.00046</td></tr><tr><td>test_acc</td><td>0.9789</td></tr><tr><td>test_loss</td><td>0.08645</td></tr><tr><td>train_accuracy</td><td>0.99573</td></tr><tr><td>train_loss</td><td>0.01391</td></tr><tr><td>val_accuracy</td><td>0.9798</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">depth_8_skip_True</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/ry51vnt0' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/ry51vnt0</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251118_090915-ry51vnt0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for depth in config[\"depths\"]:\n",
    "    for use_skip in config[\"use_skip_options\"]:\n",
    "        run_name = f\"depth_{len(depth)-2}_skip_{use_skip}\"\n",
    "        print(f\"\\n=== Starting experiment: {run_name} ===\")\n",
    "\n",
    "        # Initialize a new wandb run, assign to a group\n",
    "        wandb.init(\n",
    "            project=\"Lab-1\",\n",
    "            name=run_name,\n",
    "            group=\"mlp_depth_residual_comparison\",  # all runs belong to this group\n",
    "            config={\n",
    "                **config,\n",
    "                \"layer_sizes\": depth,\n",
    "                \"use_skip\": use_skip\n",
    "            },\n",
    "            reinit=True   # allows multiple runs in the same notebook\n",
    "        )\n",
    "\n",
    "        # ---------------------------\n",
    "        # Build model using GeneralModel\n",
    "        # ---------------------------\n",
    "        blocks = []\n",
    "        for nin, nout in zip(depth[:-2], depth[1:-1]):\n",
    "            blocks.append(\n",
    "                MLPBlock(\n",
    "                    nin,\n",
    "                    nout,\n",
    "                    hidden_layers=config[\"block_hidden_layers\"],\n",
    "                    use_skip=use_skip\n",
    "                )\n",
    "            )\n",
    "\n",
    "        classifier = nn.Linear(depth[-2], depth[-1])\n",
    "\n",
    "        model = GeneralModel(\n",
    "            blocks=blocks,\n",
    "            classifier=classifier,\n",
    "            flatten_input=True  # important for MLPs\n",
    "        ).to(device)\n",
    "        \n",
    "\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(config[\"epochs\"]):\n",
    "            train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
    "            val_loss, val_acc, val_report = evaluate(model, val_loader, device)\n",
    "\n",
    "            # -------------------------------\n",
    "            # Gradient norms analysis\n",
    "            # -------------------------------\n",
    "            # Take a single batch for gradient check\n",
    "            x_batch, y_batch = next(iter(train_loader))\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            model.zero_grad()\n",
    "            logits = model(x_batch)\n",
    "            loss = F.cross_entropy(logits, y_batch)\n",
    "            loss.backward()\n",
    "            grad_norms = [p.grad.norm().item() for p in model.parameters() if p.grad is not None]\n",
    "\n",
    "            # First layer gradient norm (usually the first Linear layer)\n",
    "            first_layer_grad_norm = None\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    first_layer_grad_norm = p.grad.norm().item()\n",
    "                    break  # take only the first parameter's grad\n",
    "\n",
    "            wandb.log({\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"train_accuracy\": train_acc,\n",
    "                \"val_accuracy\": val_acc,\n",
    "                \"grad_norm_mean\": np.mean(grad_norms),\n",
    "                \"grad_norm_max\": np.max(grad_norms),\n",
    "                \"grad_norm_min\": np.min(grad_norms),\n",
    "                \"grad_first_layer\": first_layer_grad_norm\n",
    "                }, step=epoch)\n",
    "\n",
    "            print(f\"[{run_name}] Epoch {epoch}\")\n",
    "                  #f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, \"\n",
    "                  #f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "        \n",
    "        # Final test evaluation\n",
    "        test_loss, test_acc, test_report = evaluate(model, dl_test, device)\n",
    "        wandb.log({\"test_loss\": test_loss, \"test_acc\": test_acc})\n",
    "        wandb.log({\"classification_report\": str(test_report)})\n",
    "        # Finish this run\n",
    "        wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c75a80",
   "metadata": {},
   "source": [
    "#### Results\n",
    "For MNIST, a relatively simple dataset, shallow architectures (2 and 4 MLP blocks) achieve similarly good performance. In these cases, vanishing gradients are not noticeable, as confirmed by the gradient norms logged in WandB. However, architectures without skip connections still show slower convergence, suggesting that either mild gradient attenuation occurs or that skip connections provide additional architectural flexibility by allowing the network to partially bypass layers that are not contributing useful features.\n",
    "\n",
    "The impact of skip connections becomes dramatic in the deeper 8-block architecture. Without skip connections, the network struggles to learn, likely due to severe vanishing gradients. Introducing skip connections enables effective gradient flow and allows the model to achieve good performance, demonstrating the importance of residual connections in facilitating training in deeper networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59bdd8-3377-4311-b45f-511c2fb0b53e",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Rinse and Repeat (but with a CNN)\n",
    "\n",
    "Repeat the verification you did above, but with **Convolutional** Neural Networks. If you were careful about abstracting your model and training code, this should be a simple exercise. Show that **deeper** CNNs *without* residual connections do not always work better and **even deeper** ones *with* residual connections.\n",
    "\n",
    "**Hint**: You probably should do this exercise using CIFAR-10, since MNIST is *very* easy (at least up to about 99% accuracy).\n",
    "\n",
    "**Tip**: Feel free to reuse the ResNet building blocks defined in `torchvision.models.resnet` (e.g. [BasicBlock](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L59) which handles the cascade of 3x3 convolutions, skip connections, and optional downsampling). This is an excellent exercise in code diving. \n",
    "\n",
    "**Spoiler**: Depending on the optional exercises you plan to do below, you should think *very* carefully about the architectures of your CNNs here (so you can reuse them!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad6876d",
   "metadata": {},
   "source": [
    "#### Non-Residual BasicBlock\n",
    "Since I'm going to use BasicBlock from trochvison.model.resnet for this experiment I define a Carbon copy of the BasicBlock class that dosen't have the residual connection (since the residual connection is not present the optional downsampling layer is not needed as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee73b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Callable\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "class NonResidualBasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        \n",
    "        # Two convolutions (same as BasicBlock)\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)  # just a final ReLU, no skip connection\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e98faa9",
   "metadata": {},
   "source": [
    "#### Data preparation\n",
    "I'm going to use CIFAR10 dataset for this experiment, and I will devide it into train and validation dataset with a 90/10 split ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8baa0e-b17f-4a77-8a88-dadfdc6763ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 45000, Val size: 5000, Test size: 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = {\n",
    "    \"batch_size\": 128,\n",
    "    \"lr\": 0.1,\n",
    "    \"epochs\": 10,\n",
    "}\n",
    "\n",
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "# Download CIFAR-10 dataset\n",
    "full_train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                                  download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                            download=True, transform=transform)\n",
    "\n",
    "# Split train into train + validation\n",
    "val_size = int(0.1 *len(full_train_dataset))\n",
    "train_size = len(full_train_dataset) - val_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}, Test size: {len(test_dataset)}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c8c6cd",
   "metadata": {},
   "source": [
    "### Models definition\n",
    "For this experiment I'm going to define and use 8 different model architectures four of them will use residual connections the other four will be a copy of the first four but will NOT use residual connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c007180",
   "metadata": {},
   "source": [
    "#### Residual Models\n",
    "I'm going to define four residual models with increasing depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff512ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import BasicBlock\n",
    "\n",
    "# --- 4-block residual ---\n",
    "res_4_blocks = GeneralModel(\n",
    "    blocks=[\n",
    "        nn.Sequential(nn.Conv2d(3, 64, 1, 1, bias=False), nn.BatchNorm2d(64), nn.ReLU()),\n",
    "        BasicBlock(64, 64),\n",
    "        BasicBlock(64, 64),\n",
    "        BasicBlock(64, 128, stride=2, downsample=nn.Sequential(nn.Conv2d(64,128,1,2,bias=False), nn.BatchNorm2d(128))),\n",
    "        BasicBlock(128, 128),\n",
    "    ],\n",
    "    classifier=nn.Linear(128*16*16, 10),\n",
    "    flatten_input=False\n",
    ")\n",
    "\n",
    "# --- 8-block residual ---\n",
    "res_8_blocks = GeneralModel(\n",
    "    blocks=[\n",
    "        nn.Sequential(nn.Conv2d(3, 64, 1, 1, bias=False), nn.BatchNorm2d(64), nn.ReLU()),\n",
    "        BasicBlock(64, 64), BasicBlock(64, 64), BasicBlock(64, 64), BasicBlock(64, 64),\n",
    "        BasicBlock(64, 128, stride=2, downsample=nn.Sequential(nn.Conv2d(64,128,1,2,bias=False), nn.BatchNorm2d(128))),\n",
    "        BasicBlock(128, 128), BasicBlock(128, 128), BasicBlock(128, 128),\n",
    "    ],\n",
    "    classifier=nn.Linear(128*16*16, 10),\n",
    "    flatten_input=False\n",
    ")\n",
    "\n",
    "res_16_blocks = GeneralModel(\n",
    "    blocks=[\n",
    "        nn.Sequential(nn.Conv2d(3,64,1,1,bias=False), nn.BatchNorm2d(64), nn.ReLU()),\n",
    "\n",
    "        # First 8 blocks, 64 channels\n",
    "        BasicBlock(64,64), BasicBlock(64,64), BasicBlock(64,64), BasicBlock(64,64),\n",
    "        BasicBlock(64,64), BasicBlock(64,64), BasicBlock(64,64), BasicBlock(64,64),\n",
    "\n",
    "        # Next 8 blocks, 128 channels, downsample at the first of these\n",
    "        BasicBlock(64,128,stride=2, downsample=nn.Sequential(nn.Conv2d(64,128,1,2,bias=False), nn.BatchNorm2d(128))),\n",
    "        BasicBlock(128,128), BasicBlock(128,128), BasicBlock(128,128),\n",
    "        BasicBlock(128,128), BasicBlock(128,128), BasicBlock(128,128), BasicBlock(128,128),\n",
    "    ],\n",
    "    classifier=nn.Linear(128*16*16,10),\n",
    "    flatten_input=False\n",
    ")\n",
    "\n",
    "res_32_blocks_64_128_256 = GeneralModel(\n",
    "    blocks=[\n",
    "        nn.Sequential(nn.Conv2d(3,64,1,1,bias=False), nn.BatchNorm2d(64), nn.ReLU()),\n",
    "\n",
    "        # 16 blocks, 64 channels\n",
    "        *[BasicBlock(64,64) for _ in range(16)],\n",
    "\n",
    "        # 8 blocks, 128 channels, downsample at first\n",
    "        BasicBlock(64,128,stride=2, downsample=nn.Sequential(nn.Conv2d(64,128,1,2,bias=False), nn.BatchNorm2d(128))),\n",
    "        *[BasicBlock(128,128) for _ in range(7)],\n",
    "\n",
    "        # 8 blocks, 256 channels, downsample at first\n",
    "        BasicBlock(128,256,stride=2, downsample=nn.Sequential(nn.Conv2d(128,256,1,2,bias=False), nn.BatchNorm2d(256))),\n",
    "        *[BasicBlock(256,256) for _ in range(7)],\n",
    "    ],\n",
    "    classifier=nn.Linear(256*8*8,10),  # final spatial size 8x8\n",
    "    flatten_input=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aaea3b",
   "metadata": {},
   "source": [
    "#### Non-Residual Models\n",
    "These models are analouge to the previus but without residual connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e4acc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2-block non-residual ---\n",
    "nores_2_blocks = GeneralModel(\n",
    "    blocks=[\n",
    "        nn.Sequential(nn.Conv2d(3,64,1,1,bias=False), nn.BatchNorm2d(64), nn.ReLU()),\n",
    "        NonResidualBasicBlock(64, 64),\n",
    "        NonResidualBasicBlock(64, 128, stride=2),\n",
    "    ],\n",
    "    classifier=nn.Linear(128*16*16, 10),\n",
    "    flatten_input=False\n",
    ")\n",
    "\n",
    "# --- 4-block non-residual ---\n",
    "nores_4_blocks = GeneralModel(\n",
    "    blocks=[\n",
    "        nn.Sequential(nn.Conv2d(3,64,1,1,bias=False), nn.BatchNorm2d(64), nn.ReLU()),\n",
    "        NonResidualBasicBlock(64, 64),\n",
    "        NonResidualBasicBlock(64, 64),\n",
    "        NonResidualBasicBlock(64, 128, stride=2),\n",
    "        NonResidualBasicBlock(128, 128),\n",
    "    ],\n",
    "    classifier=nn.Linear(128*16*16, 10),\n",
    "    flatten_input=False\n",
    ")\n",
    "\n",
    "# --- 8-block non-residual ---\n",
    "nores_8_blocks = GeneralModel(\n",
    "    blocks=[\n",
    "        nn.Sequential(nn.Conv2d(3,64,1,1,bias=False), nn.BatchNorm2d(64), nn.ReLU()),\n",
    "        NonResidualBasicBlock(64,64), NonResidualBasicBlock(64,64), NonResidualBasicBlock(64,64), NonResidualBasicBlock(64,64),\n",
    "        NonResidualBasicBlock(64,128,stride=2), NonResidualBasicBlock(128,128), NonResidualBasicBlock(128,128), NonResidualBasicBlock(128,128)\n",
    "    ],\n",
    "    classifier=nn.Linear(128*16*16, 10),\n",
    "    flatten_input=False\n",
    ")\n",
    "\n",
    "nores_16_blocks = GeneralModel(\n",
    "    blocks=[\n",
    "        nn.Sequential(nn.Conv2d(3,64,1,1,bias=False), nn.BatchNorm2d(64), nn.ReLU()),\n",
    "\n",
    "        # First 8 blocks, 64 channels\n",
    "        NonResidualBasicBlock(64,64), NonResidualBasicBlock(64,64), NonResidualBasicBlock(64,64), NonResidualBasicBlock(64,64),\n",
    "        NonResidualBasicBlock(64,64), NonResidualBasicBlock(64,64), NonResidualBasicBlock(64,64), NonResidualBasicBlock(64,64),\n",
    "\n",
    "        # Next 8 blocks, 128 channels, downsample at first of these\n",
    "        NonResidualBasicBlock(64,128,stride=2), NonResidualBasicBlock(128,128), NonResidualBasicBlock(128,128), NonResidualBasicBlock(128,128),\n",
    "        NonResidualBasicBlock(128,128), NonResidualBasicBlock(128,128), NonResidualBasicBlock(128,128), NonResidualBasicBlock(128,128),\n",
    "    ],\n",
    "    classifier=nn.Linear(128*16*16,10),\n",
    "    flatten_input=False\n",
    ")\n",
    "\n",
    "nores_32_blocks_64_128_256 = GeneralModel(\n",
    "    blocks=[\n",
    "        nn.Sequential(nn.Conv2d(3,64,1,1,bias=False), nn.BatchNorm2d(64), nn.ReLU()),\n",
    "\n",
    "        # 16 blocks, 64 channels\n",
    "        *[NonResidualBasicBlock(64,64) for _ in range(16)],\n",
    "\n",
    "        # 8 blocks, 128 channels, downsample at first\n",
    "        NonResidualBasicBlock(64,128,stride=2),\n",
    "        *[NonResidualBasicBlock(128,128) for _ in range(7)],\n",
    "\n",
    "        # 8 blocks, 256 channels, downsample at first\n",
    "        NonResidualBasicBlock(128,256,stride=2),\n",
    "        *[NonResidualBasicBlock(256,256) for _ in range(7)],\n",
    "    ],\n",
    "    classifier=nn.Linear(256*8*8,10),  # final spatial size 8x8\n",
    "    flatten_input=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1566a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"Res_4\", res_4_blocks),\n",
    "    (\"Res_8\", res_8_blocks),\n",
    "    (\"Res_16\", res_16_blocks),\n",
    "    (\"Res_32_64_128_256\", res_32_blocks_64_128_256),\n",
    "    (\"NoRes_4\", nores_4_blocks),\n",
    "    (\"NoRes_8\", nores_8_blocks),\n",
    "    (\"NoRes_16\", nores_16_blocks),\n",
    "    (\"NoRes_32_64_128_256\", nores_32_blocks_64_128_256),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f0022",
   "metadata": {},
   "source": [
    "#### Training, Testing and Logging of the eight models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce40f59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training model: Res_16 ===\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/andromeda/personal/mpiras/Lab_1/wandb/run-20251118_130417-9hxwtlq5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/9hxwtlq5' target=\"_blank\">cnn_Res_16</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/9hxwtlq5' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/9hxwtlq5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>grad_first_layer</td><td>▃▃█▃▂▂▂▁▁▂</td></tr><tr><td>grad_norm_max</td><td>█▄▇▂▁▂▁▁▁▂</td></tr><tr><td>grad_norm_mean</td><td>█▇█▄▂▂▂▂▁▃</td></tr><tr><td>grad_norm_min</td><td>▁▆█▄▂▃▃▃▃▇</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄▅▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇██████</td></tr><tr><td>val_loss</td><td>█▅▄▁▁▁▁▁▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>classification_report</td><td>              precis...</td></tr><tr><td>grad_first_layer</td><td>0.47147</td></tr><tr><td>grad_norm_max</td><td>1.91021</td></tr><tr><td>grad_norm_mean</td><td>0.21116</td></tr><tr><td>grad_norm_min</td><td>0.01076</td></tr><tr><td>test_acc</td><td>0.7538</td></tr><tr><td>test_loss</td><td>0.94612</td></tr><tr><td>train_accuracy</td><td>0.94329</td></tr><tr><td>train_loss</td><td>0.15984</td></tr><tr><td>val_accuracy</td><td>0.7536</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cnn_Res_16</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/9hxwtlq5' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/9hxwtlq5</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251118_130417-9hxwtlq5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training model: Res_32_64_128_256 ===\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/andromeda/personal/mpiras/Lab_1/wandb/run-20251118_131015-vnmxt4u9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/vnmxt4u9' target=\"_blank\">cnn_Res_32_64_128_256</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/vnmxt4u9' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/vnmxt4u9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>grad_first_layer</td><td>▁█▁▂▁▁▁▁▁▁</td></tr><tr><td>grad_norm_max</td><td>▁█▁▅▁▁▁▁▁▁</td></tr><tr><td>grad_norm_mean</td><td>▁█▁▃▁▁▁▁▁▁</td></tr><tr><td>grad_norm_min</td><td>▃█▁▄▁▂▁▁▂▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁▄▆▆▇▇█▇▇█</td></tr><tr><td>train_loss</td><td>█▅▃▃▂▂▁▂▂▁</td></tr><tr><td>val_accuracy</td><td>▇▃█▁▇▇█▇▇█</td></tr><tr><td>val_loss</td><td>▁█▁▃▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>classification_report</td><td>              precis...</td></tr><tr><td>grad_first_layer</td><td>0.31276</td></tr><tr><td>grad_norm_max</td><td>0.74001</td></tr><tr><td>grad_norm_mean</td><td>0.05331</td></tr><tr><td>grad_norm_min</td><td>0.00129</td></tr><tr><td>test_acc</td><td>0.7837</td></tr><tr><td>test_loss</td><td>1.01633</td></tr><tr><td>train_accuracy</td><td>0.97871</td></tr><tr><td>train_loss</td><td>0.06198</td></tr><tr><td>val_accuracy</td><td>0.8052</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cnn_Res_32_64_128_256</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/vnmxt4u9' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/vnmxt4u9</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251118_131015-vnmxt4u9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training model: NoRes_16 ===\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/andromeda/personal/mpiras/Lab_1/wandb/run-20251118_132114-5sejz06b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/5sejz06b' target=\"_blank\">cnn_NoRes_16</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/5sejz06b' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/5sejz06b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>grad_first_layer</td><td>▂▁▂▃█▃▂▆▂▇</td></tr><tr><td>grad_norm_max</td><td>▄▃▃▃█▃▂▅▁█</td></tr><tr><td>grad_norm_mean</td><td>▅▁▃▂█▂▁▅▁▇</td></tr><tr><td>grad_norm_min</td><td>▂▁▄▂█▄▁█▃█</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄▅▆▆▇███</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▅▅▆▇▇█▇</td></tr><tr><td>val_loss</td><td>█▆▅▄▄▃▂▃▁▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>classification_report</td><td>              precis...</td></tr><tr><td>grad_first_layer</td><td>1.43009</td></tr><tr><td>grad_norm_max</td><td>3.79934</td></tr><tr><td>grad_norm_mean</td><td>0.80462</td></tr><tr><td>grad_norm_min</td><td>0.11739</td></tr><tr><td>test_acc</td><td>0.5865</td></tr><tr><td>test_loss</td><td>1.32164</td></tr><tr><td>train_accuracy</td><td>0.64511</td></tr><tr><td>train_loss</td><td>0.99864</td></tr><tr><td>val_accuracy</td><td>0.593</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cnn_NoRes_16</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/5sejz06b' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/5sejz06b</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251118_132114-5sejz06b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training model: NoRes_32_64_128_256 ===\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/andromeda/personal/mpiras/Lab_1/wandb/run-20251118_132645-jxg19q5u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/jxg19q5u' target=\"_blank\">cnn_NoRes_32_64_128_256</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/jxg19q5u' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/jxg19q5u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>grad_first_layer</td><td>▂▁▂█▂▂▁▁▂▂</td></tr><tr><td>grad_norm_max</td><td>▂▁▃█▂▂▁▂▂▂</td></tr><tr><td>grad_norm_mean</td><td>▂▁▂█▃▃▂▁▁▃</td></tr><tr><td>grad_norm_min</td><td>▃▁▃█▄▄▂▁▂▄</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▆▆▅▄▄▃▂▂▁</td></tr><tr><td>val_accuracy</td><td>▁▅▄▁▅▄▆██▇</td></tr><tr><td>val_loss</td><td>▅▃▃█▂▄▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>classification_report</td><td>              precis...</td></tr><tr><td>grad_first_layer</td><td>0.72478</td></tr><tr><td>grad_norm_max</td><td>0.88451</td></tr><tr><td>grad_norm_mean</td><td>0.31876</td></tr><tr><td>grad_norm_min</td><td>0.08772</td></tr><tr><td>test_acc</td><td>0.4205</td></tr><tr><td>test_loss</td><td>1.56177</td></tr><tr><td>train_accuracy</td><td>0.44904</td></tr><tr><td>train_loss</td><td>1.47316</td></tr><tr><td>val_accuracy</td><td>0.4272</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cnn_NoRes_32_64_128_256</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/jxg19q5u' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/jxg19q5u</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251118_132645-jxg19q5u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary ===\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for name, model in models:\n",
    "    print(f\"\\n=== Training model: {name} ===\")\n",
    "\n",
    "    wandb.init(\n",
    "            project=\"Lab-1\",\n",
    "            name=f\"cnn_{name}\",\n",
    "            group=\"cnn_residual_comparison_exp2\",  # all runs belong to this group\n",
    "            config={\n",
    "                **config,\n",
    "                \"BasicBlock_depth\": model.blocks.__len__()-1,\n",
    "                \"optimizer\": \"Adam\",\n",
    "                \"lr\": 0.001,\n",
    "            },\n",
    "            reinit=True   # allows multiple runs in the same notebook\n",
    "        )\n",
    "\n",
    "    model = model.to(device)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=0.9, weight_decay=5e-4)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        val_loss, val_acc, _ = evaluate(model, val_loader, device)\n",
    "\n",
    "        # -------------------------------\n",
    "        # Gradient norms analysis\n",
    "        # -------------------------------\n",
    "        # Take a single batch for gradient check\n",
    "        x_batch, y_batch = next(iter(train_loader))\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        model.zero_grad()\n",
    "        logits = model(x_batch)\n",
    "        loss = F.cross_entropy(logits, y_batch)\n",
    "        loss.backward()\n",
    "        grad_norms = [p.grad.norm().item() for p in model.parameters() if p.grad is not None]\n",
    "\n",
    "        # First layer gradient norm (usually the first Linear layer)\n",
    "        first_layer_grad_norm = None\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                first_layer_grad_norm = p.grad.norm().item()\n",
    "                break  # take only the first parameter's grad\n",
    "\n",
    "        wandb.log({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"val_accuracy\": val_acc,\n",
    "            \"grad_norm_mean\": np.mean(grad_norms),\n",
    "            \"grad_norm_max\": np.max(grad_norms),\n",
    "            \"grad_norm_min\": np.min(grad_norms),\n",
    "            \"grad_first_layer\": first_layer_grad_norm\n",
    "            }, step=epoch)\n",
    "    \n",
    "    # Final test evaluation\n",
    "    test_loss, test_acc, test_report = evaluate(model, test_loader, device)\n",
    "    wandb.log({\"test_loss\": test_loss, \"test_acc\": test_acc})\n",
    "    wandb.log({\"classification_report\": str(test_report)})\n",
    "    # Finish this run\n",
    "    wandb.finish()\n",
    "\n",
    "# -------------------------------\n",
    "# Final summary\n",
    "# -------------------------------\n",
    "print(\"\\n=== Summary ===\")\n",
    "for name, res in results.items():\n",
    "    print(f\"{name}: test_loss={res['test_loss']:.4f}, test_acc={res['test_acc']:.4f}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589f94b1",
   "metadata": {},
   "source": [
    "#### Results\n",
    "Deeper residual models yeld higher performances in train set, this still apply but to a lesser extent in validation and test, this can be attributed to overfitting in fact we can see a performance discrepancy in train and test performances especially for deeper models where we see around 15% accuracy loss, usually this problem is mitigated or outright solved by data augmentation that notably I did not use here beeing the focus of the experiment more on seeing the effect of not using residual connection more than obtaining the best performances in testing in fact we can see that non residual models inversly show worse performances the deeper the model is, looking at the gradient norms, especially at the mean and the min I do notice that the norms of the models with no residual connections tend to be higher than the norms of the models with the residual connection suggesting that exploding gradient is occurring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4de2f2-abc5-4f98-9eaf-3497f734a022",
   "metadata": {},
   "source": [
    "-----\n",
    "## Exercise 2: Choose at Least One\n",
    "\n",
    "Below are **three** exercises that ask you to deepen your understanding of Deep Networks for visual recognition. You must choose **at least one** of the below for your final submission -- feel free to do **more**, but at least **ONE** you must submit. Each exercise is designed to require you to dig your hands **deep** into the guts of your models in order to do new and interesting things.\n",
    "\n",
    "**Note**: These exercises are designed to use your small, custom CNNs and small datasets. This is to keep training times reasonable. If you have a decent GPU, feel free to use pretrained ResNets and larger datasets (e.g. the [Imagenette](https://pytorch.org/vision/0.20/generated/torchvision.datasets.Imagenette.html#torchvision.datasets.Imagenette) dataset at 160px)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07978e8e-9f2e-4949-9699-495af6cb6349",
   "metadata": {},
   "source": [
    "### Exercise 2.1: *Fine-tune* a pre-trained model\n",
    "Train one of your residual CNN models from Exercise 1.3 on CIFAR-10. Then:\n",
    "1. Use the pre-trained model as a **feature extractor** (i.e. to extract the feature activations of the layer input into the classifier) on CIFAR-100. Use a **classical** approach (e.g. Linear SVM, K-Nearest Neighbor, or Bayesian Generative Classifier) from scikit-learn to establish a **stable baseline** performance on CIFAR-100 using the features extracted using your CNN.\n",
    "2. Fine-tune your CNN on the CIFAR-100 training set and compare with your stable baseline. Experiment with different strategies:\n",
    "    - Unfreeze some of the earlier layers for fine-tuning.\n",
    "    - Test different optimizers (Adam, SGD, etc.).\n",
    "\n",
    "Each of these steps will require you to modify your model definition in some way. For 1, you will need to return the activations of the last fully-connected layer (or the global average pooling layer). For 2, you will need to replace the original, 10-class classifier with a new, randomly-initialized 100-class classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "469e81a3-08ca-4549-a2f8-f47cf5a0308b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e888abf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transform = T.Compose([\n",
    "    #T.RandomCrop(32, padding=4),\n",
    "    #T.RandomHorizontalFlip(),\n",
    "    T.Resize(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR100(root=\"./data\", train=True,  download=True, transform=train_transform)\n",
    "test_set  = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(test_set,  batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624c9e8",
   "metadata": {},
   "source": [
    "#### Stable Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad2b0965",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "resnet = resnet.to(device)\n",
    "resnet.eval()\n",
    "\n",
    "# REMOVE final fully connected layer → keep everything except FC\n",
    "feature_extractor = nn.Sequential(*list(resnet.children())[:-1]).to(device)\n",
    "\n",
    "# Freeze parameters\n",
    "for p in feature_extractor.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be809f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(loader, model, device):\n",
    "    feats, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, y in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            f = model(imgs).squeeze()  # (batch, 512)\n",
    "            feats.append(f.cpu())\n",
    "            labels.append(y)\n",
    "    return torch.cat(feats), torch.cat(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1048762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (50000, 512)\n"
     ]
    }
   ],
   "source": [
    "train_feats, train_labels = extract_features(train_loader, feature_extractor, device)\n",
    "val_feats, val_labels     = extract_features(val_loader,   feature_extractor, device)\n",
    "\n",
    "# Convert to numpy for scikit-learn\n",
    "train_feats = train_feats.numpy()\n",
    "train_labels = train_labels.numpy()\n",
    "val_feats = val_feats.numpy()\n",
    "val_labels = val_labels.numpy()\n",
    "\n",
    "print(\"Feature shape:\", train_feats.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dde898b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR100 Feature Baseline Accuracy: 0.5989\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(train_feats, train_labels)\n",
    "\n",
    "acc = clf.score(val_feats, val_labels)\n",
    "print(\"CIFAR100 Feature Baseline Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89ea7b5",
   "metadata": {},
   "source": [
    "#### Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5786f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_finetuning_model():\n",
    "    model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "    \n",
    "    num_feats = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_feats, 100)  # CIFAR-100 has 100 classes\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3314ef93",
   "metadata": {},
   "source": [
    "##### Train Only Classifier Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "659de850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/andromeda/personal/mpiras/Lab_1/wandb/run-20251119_132333-nwmlhsn8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/nwmlhsn8' target=\"_blank\">no_augment_resnet18_finetune_head_only</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/nwmlhsn8' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/nwmlhsn8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEAD ONLY] Epoch 1/10  Train Acc=0.4105  Val Acc=0.5283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEAD ONLY] Epoch 2/10  Train Acc=0.5607  Val Acc=0.5574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEAD ONLY] Epoch 3/10  Train Acc=0.5952  Val Acc=0.5728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEAD ONLY] Epoch 4/10  Train Acc=0.6117  Val Acc=0.5755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEAD ONLY] Epoch 5/10  Train Acc=0.6257  Val Acc=0.5917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEAD ONLY] Epoch 6/10  Train Acc=0.6353  Val Acc=0.5869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEAD ONLY] Epoch 7/10  Train Acc=0.6441  Val Acc=0.5884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEAD ONLY] Epoch 8/10  Train Acc=0.6501  Val Acc=0.5951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEAD ONLY] Epoch 9/10  Train Acc=0.6541  Val Acc=0.5939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEAD ONLY] Epoch 10/10  Train Acc=0.6595  Val Acc=0.5903\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆█▇▇██▇</td></tr><tr><td>val_loss</td><td>█▄▃▂▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>0.65952</td></tr><tr><td>train_loss</td><td>1.20296</td></tr><tr><td>val_accuracy</td><td>0.5903</td></tr><tr><td>val_loss</td><td>1.49114</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">no_augment_resnet18_finetune_head_only</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/nwmlhsn8' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/nwmlhsn8</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251119_132333-nwmlhsn8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = get_finetuning_model()\n",
    "\n",
    "# Freeze entire backbone\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze only the classifier\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "config = {\n",
    "    \"batch_size\": 128,\n",
    "    \"Adam_lr\": 1e-3,\n",
    "    \"SGD_lr\": 0.01,\n",
    "    \"epochs\": 10,\n",
    "    \"optimizer\": \"Adam\",\n",
    "}\n",
    "\n",
    "if config[\"optimizer\"] == \"Adam\":\n",
    "    optimizer = optim.Adam(model.fc.parameters(), lr=config[\"Adam_lr\"])\n",
    "elif config[\"optimizer\"] == \"SGD\":\n",
    "    optimizer = optim.SGD(model.fc.parameters(), lr=config[\"SGD_lr\"], momentum=0.9)\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "            project=\"Lab-1\",\n",
    "            name=\"no_augment_resnet18_finetune_head_only\",\n",
    "            group=\"fine_tuning_experiments\",  # all runs belong to this group\n",
    "            config={\n",
    "                **config,\n",
    "            },\n",
    "            reinit=True   # allows multiple runs in the same notebook\n",
    "        )\n",
    "\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss, val_acc, _ = evaluate(model, val_loader, device)\n",
    "    wandb.log({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"train_accuracy\": train_acc,\n",
    "        \"val_accuracy\": val_acc\n",
    "    }, step=epoch)\n",
    "    print(f\"[HEAD ONLY] Epoch {epoch+1}/{config[\"epochs\"]}  Train Acc={train_acc:.4f}  Val Acc={val_acc:.4f}\")\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c141e",
   "metadata": {},
   "source": [
    "#### Train Classifaier head and last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de8175bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/andromeda/personal/mpiras/Lab_1/wandb/run-20251119_133230-2h9njq39</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/2h9njq39' target=\"_blank\">no_augment_resnet18_finetune_layer4_and_head</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/2h9njq39' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/2h9njq39</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNFREEZE layer4] Epoch 1/10  Train Acc=0.5504  Val Acc=0.6774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNFREEZE layer4] Epoch 2/10  Train Acc=0.7748  Val Acc=0.7056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNFREEZE layer4] Epoch 3/10  Train Acc=0.8948  Val Acc=0.7144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNFREEZE layer4] Epoch 4/10  Train Acc=0.9708  Val Acc=0.7211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNFREEZE layer4] Epoch 5/10  Train Acc=0.9953  Val Acc=0.7262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNFREEZE layer4] Epoch 6/10  Train Acc=0.9989  Val Acc=0.7306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNFREEZE layer4] Epoch 7/10  Train Acc=0.9994  Val Acc=0.7324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNFREEZE layer4] Epoch 8/10  Train Acc=0.9996  Val Acc=0.7336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNFREEZE layer4] Epoch 9/10  Train Acc=0.9986  Val Acc=0.7195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNFREEZE layer4] Epoch 10/10  Train Acc=0.9940  Val Acc=0.6974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁▄▆███████</td></tr><tr><td>train_loss</td><td>█▄▃▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▆▇███▆▃</td></tr><tr><td>val_loss</td><td>▃▁▁▂▂▂▃▃▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>0.99396</td></tr><tr><td>train_loss</td><td>0.02662</td></tr><tr><td>val_accuracy</td><td>0.6974</td></tr><tr><td>val_loss</td><td>1.40907</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">no_augment_resnet18_finetune_layer4_and_head</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/2h9njq39' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/2h9njq39</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251119_133230-2h9njq39/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = get_finetuning_model()\n",
    "\n",
    "# Freeze all layers first\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze last convolutional block\n",
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "# Unfreeze classifier\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "config = {\n",
    "    \"batch_size\": 128,\n",
    "    \"Adam_lr_layer4\": 1e-4,\n",
    "    \"Adam_lr_fc\": 1e-3,\n",
    "    \"SGD_lr_layer4\": 0.001,\n",
    "    \"SGD_lr_fc\": 0.01,\n",
    "    \"epochs\": 10,\n",
    "    \"optimizer\": \"Adam\",\n",
    "}\n",
    "\n",
    "if config[\"optimizer\"] == \"Adam\":\n",
    "    optimizer = optim.Adam([\n",
    "        {\"params\": model.layer4.parameters(), \"lr\": config[\"Adam_lr_layer4\"]},\n",
    "        {\"params\": model.fc.parameters(),      \"lr\": config[\"Adam_lr_fc\"]},\n",
    "    ])\n",
    "elif config[\"optimizer\"] == \"SGD\":\n",
    "    optimizer = optim.SGD([\n",
    "        {\"params\": model.layer4.parameters(), \"lr\": config[\"SGD_lr_layer4\"], \"momentum\": 0.9},\n",
    "        {\"params\": model.fc.parameters(),      \"lr\": config[\"SGD_lr_fc\"],      \"momentum\": 0.9},\n",
    "    ])\n",
    "\n",
    "wandb.init(\n",
    "            project=\"Lab-1\",\n",
    "            name=\"no_augment_resnet18_finetune_layer4_and_head\",\n",
    "            group=\"fine_tuning_experiments\",  # all runs belong to this group\n",
    "            config={\n",
    "                **config,\n",
    "            },\n",
    "            reinit=True   # allows multiple runs in the same notebook\n",
    "        )\n",
    "\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss, val_acc, _ = evaluate(model, val_loader, device)\n",
    "    wandb.log({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"train_accuracy\": train_acc,\n",
    "        \"val_accuracy\": val_acc\n",
    "    }, step=epoch)\n",
    "    print(f\"[UNFREEZE layer4] Epoch {epoch+1}/{config[\"epochs\"]}  Train Acc={train_acc:.4f}  Val Acc={val_acc:.4f}\")\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196d1d79",
   "metadata": {},
   "source": [
    "#### Full Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d4161e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatteo-piras\u001b[0m (\u001b[33mmatteo-piras-universit-di-firenze\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/andromeda/personal/mpiras/Lab_1/wandb/run-20251119_135022-zhmxbrmk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/zhmxbrmk' target=\"_blank\">no_augment_resnet18_finetune_full_model</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/zhmxbrmk' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/zhmxbrmk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FULL FT] Epoch 1/10  Train Acc=0.5599  Val Acc=0.7289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FULL FT] Epoch 2/10  Train Acc=0.8027  Val Acc=0.7717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FULL FT] Epoch 3/10  Train Acc=0.9007  Val Acc=0.7861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FULL FT] Epoch 4/10  Train Acc=0.9633  Val Acc=0.7914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FULL FT] Epoch 5/10  Train Acc=0.9904  Val Acc=0.7952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FULL FT] Epoch 6/10  Train Acc=0.9970  Val Acc=0.8004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FULL FT] Epoch 7/10  Train Acc=0.9989  Val Acc=0.8058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FULL FT] Epoch 8/10  Train Acc=0.9987  Val Acc=0.7929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FULL FT] Epoch 9/10  Train Acc=0.9900  Val Acc=0.7615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FULL FT] Epoch 10/10  Train Acc=0.9850  Val Acc=0.7803\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁▅▆▇██████</td></tr><tr><td>train_loss</td><td>█▄▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇▇██▇▄▆</td></tr><tr><td>val_loss</td><td>█▃▁▁▁▁▁▂▅▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>0.985</td></tr><tr><td>train_loss</td><td>0.06859</td></tr><tr><td>val_accuracy</td><td>0.7803</td></tr><tr><td>val_loss</td><td>0.85825</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">no_augment_resnet18_finetune_full_model</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/zhmxbrmk' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1/runs/zhmxbrmk</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Lab-1</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251119_135022-zhmxbrmk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = get_finetuning_model()\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "config = {\n",
    "    \"batch_size\": 128,\n",
    "    \"Adam_lr\": 1e-4,\n",
    "    \"SGD_lr\": 0.001,\n",
    "    \"epochs\": 10,\n",
    "    \"optimizer\": \"Adam\",\n",
    "}\n",
    "\n",
    "if config[\"optimizer\"] == \"Adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"Adam_lr\"])\n",
    "elif config[\"optimizer\"] == \"SGD\":\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config[\"SGD_lr\"], momentum=0.9)\n",
    "\n",
    "wandb.init(\n",
    "            project=\"Lab-1\",\n",
    "            name=\"no_augment_resnet18_finetune_full_model\",\n",
    "            group=\"fine_tuning_experiments\",  # all runs belong to this group\n",
    "            config={\n",
    "                **config,\n",
    "            },\n",
    "            reinit=True   # allows multiple runs in the same notebook\n",
    "        )\n",
    "\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss, val_acc, _ = evaluate(model, val_loader, device)\n",
    "    wandb.log({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"train_accuracy\": train_acc,\n",
    "        \"val_accuracy\": val_acc\n",
    "    }, step=epoch)\n",
    "    print(f\"[FULL FT] Epoch {epoch+1}/{config[\"epochs\"]}  Train Acc={train_acc:.4f}  Val Acc={val_acc:.4f}\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a3a7b-2ed6-4f58-a1b7-5ab1fc432893",
   "metadata": {},
   "source": [
    "### Exercise 2.2: *Distill* the knowledge from a large model into a smaller one\n",
    "In this exercise you will see if you can derive a *small* model that performs comparably to a larger one on CIFAR-10. To do this, you will use [Knowledge Distillation](https://arxiv.org/abs/1503.02531):\n",
    "\n",
    "> Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in a Neural Network, NeurIPS 2015.\n",
    "\n",
    "To do this:\n",
    "1. Train one of your best-performing CNNs on CIFAR-10 from Exercise 1.3 above. This will be your **teacher** model.\n",
    "2. Define a *smaller* variant with about half the number of parameters (change the width and/or depth of the network). Train it on CIFAR-10 and verify that it performs *worse* than your **teacher**. This small network will be your **student** model.\n",
    "3. Train the **student** using a combination of **hard labels** from the CIFAR-10 training set (cross entropy loss) and **soft labels** from predictions of the **teacher** (Kulback-Leibler loss between teacher and student).\n",
    "\n",
    "Try to optimize training parameters in order to maximize the performance of the student. It should at least outperform the student trained only on hard labels in Setp 2.\n",
    "\n",
    "**Tip**: You can save the predictions of the trained teacher network on the training set and adapt your dataloader to provide them together with hard labels. This will **greatly** speed up training compared to performing a forward pass through the teacher for each batch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e33c912-0716-44ef-a91b-47ca19a2b2cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8243f811-8227-4c6f-b07f-56e8cd91643a",
   "metadata": {},
   "source": [
    "### Exercise 2.3: *Explain* the predictions of a CNN\n",
    "\n",
    "Use the CNN model you trained in Exercise 1.3 and implement [*Class Activation Maps*](http://cnnlocalization.csail.mit.edu/#:~:text=A%20class%20activation%20map%20for,decision%20made%20by%20the%20CNN.):\n",
    "\n",
    "> B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning Deep Features for Discriminative Localization. CVPR'16 (arXiv:1512.04150, 2015).\n",
    "\n",
    "Use your CNN implementation to demonstrate how your trained CNN *attends* to specific image features to recognize *specific* classes. Try your implementation out using a pre-trained ResNet-18 model and some images from the [Imagenette](https://pytorch.org/vision/0.20/generated/torchvision.datasets.Imagenette.html#torchvision.datasets.Imagenette) dataset -- I suggest you start with the low resolution version of images at 160px."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d634a700-56c2-48fd-96e0-4c94d1bd0cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
